
This repository gives the code for reproducing the experiments in Grosse and Salakhutdinov, 2015, "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix". The code is not optimized for low-level efficiency issues. 

To train the RBM:

   from the command line:

        python experiments/from_scratch.py mnist_full/sgd

   Alternatively, in interactive mode, with visualizations and likelihood evaluations as training progresses:

        from experiments import from_scratch
        from_scratch.run('mnist_full/sgd', True)

To estimate the RBM partition function and save approximate samples (by running AIS followed by a lot of Gibbs steps):

   python experiments/evaluation.py from_scratch/mnist_full/sgd all

To plot the log-likelihoods as a function of time:

   from experiments import plotting
   plotting.show_comparison('mnist_full', ['sgd'], 'test')

To plot the approximate samples and save them to <config.FIGURES_DIR>/evaluation/from_scratch/:

   from experiments import evaluation
   evaluation.save_figures('from_scratch/mnist_full/sgd')





